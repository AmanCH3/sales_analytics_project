{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Tuple, Optional, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Setup Logging\n",
    "# -------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[logging.StreamHandler()])\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Paths\n",
    "# -------------------------------------------------------\n",
    "# *** IMPORTANT: This path must be correct for the script to run ***\n",
    "FEATURE_PATH = Path(\"data/raw/nepal_electronics_transactions_6000_with_names.csv\")\n",
    "\n",
    "OUT_DIR = Path(\"data/processed\")\n",
    "MODEL_DIR = Path(\"models\")\n",
    "CACHE_PATH = MODEL_DIR / \"txn_matrix_cache.pkl\"\n",
    "RULES_PATH = MODEL_DIR / \"apriori_rules.pkl\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Data Quality & Validation\n",
    "# -------------------------------------------------------\n",
    "def validate_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger.info(\"Validating input data...\")\n",
    "    required_cols = ['transaction_id', 'product_name']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    df = df.dropna(subset=['transaction_id', 'product_name'])\n",
    "    df = df[df['product_name'].str.strip() != '']\n",
    "    if len(df) < 10:\n",
    "        raise ValueError(\"Insufficient data: need at least 10 transactions\")\n",
    "    logger.info(f\"âœ“ Data validation complete: {len(df)} rows validated\")\n",
    "    return df\n",
    "\n",
    "def print_data_quality_report(df: pd.DataFrame):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“Š DATA QUALITY REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total rows: {len(df):,}\")\n",
    "    print(f\"Unique transactions: {df['transaction_id'].nunique():,}\")\n",
    "    print(f\"Unique products: {df['product_name'].nunique():,}\")\n",
    "    items_per_txn = df.groupby('transaction_id').size()\n",
    "    print(f\"Avg items per transaction: {items_per_txn.mean():.2f}\")\n",
    "    product_freq = df['product_name'].value_counts()\n",
    "    print(f\"Most common product: {product_freq.index[0]} ({product_freq.iloc[0]} times)\")\n",
    "    if 'category' in df.columns:\n",
    "         print(f\"Unique categories: {df['category'].nunique():,}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Build Transaction Matrix\n",
    "# -------------------------------------------------------\n",
    "def create_transaction_matrix(df: pd.DataFrame, use_cache: bool = True) -> pd.DataFrame:\n",
    "    if use_cache and CACHE_PATH.exists():\n",
    "        logger.info(\"Loading cached transaction matrix...\")\n",
    "        try:\n",
    "            return joblib.load(CACHE_PATH)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Cache load failed: {e}. Rebuilding matrix...\")\n",
    "    logger.info(\"Building transaction matrix...\")\n",
    "    df[\"qty\"] = 1\n",
    "    txn = df.pivot_table(index=\"transaction_id\", columns=\"product_name\", values=\"qty\", aggfunc=\"sum\", fill_value=0)\n",
    "    txn = txn.astype(bool)\n",
    "    if use_cache:\n",
    "        MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        joblib.dump(txn, CACHE_PATH)\n",
    "        logger.info(\"âœ“ Transaction matrix cached\")\n",
    "    return txn\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Run Apriori + Association Rules\n",
    "# -------------------------------------------------------\n",
    "def run_apriori(txn_matrix: pd.DataFrame, min_support: float = 0.01, min_confidence: float = 0.2, max_len: Optional[int] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    logger.info(f\"Running Apriori (support={min_support}, confidence={min_confidence})...\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "        frequent_itemsets = apriori(txn_matrix, min_support=min_support, max_len=max_len, use_colnames=True)\n",
    "    if len(frequent_itemsets) == 0:\n",
    "        logger.warning(\"No frequent itemsets found. Try lowering 'min_support'.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    logger.info(f\"âœ“ Found {len(frequent_itemsets)} itemsets, {len(rules)} rules\")\n",
    "    return frequent_itemsets, rules\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Hierarchical Product + Category Recommendations\n",
    "# -------------------------------------------------------\n",
    "def generate_hierarchical_recommendations(df: pd.DataFrame, rules: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if 'category' not in df.columns or len(rules) == 0:\n",
    "        logger.warning(\"Skipping hierarchical recommendations: 'category' column not found.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    prod_cat_map = df.groupby('product_name')['category'].first().to_dict()\n",
    "    reco = []\n",
    "    cat_summary = {}\n",
    "    for _, row in rules.iterrows():\n",
    "        ant_list = list(row['antecedents'])\n",
    "        cons_list = list(row['consequents'])\n",
    "        for p_from in ant_list:\n",
    "            for p_to in cons_list:\n",
    "                cat_from = prod_cat_map.get(p_from)\n",
    "                cat_to = prod_cat_map.get(p_to)\n",
    "                if not cat_from or not cat_to:\n",
    "                    continue\n",
    "                type_assoc = 'same_category' if cat_from == cat_to else 'cross_category'\n",
    "                reco.append({\n",
    "                    \"product\": p_from,\n",
    "                    \"recommended_product\": p_to,\n",
    "                    \"product_category\": cat_from,\n",
    "                    \"recommended_category\": cat_to,\n",
    "                    \"type\": type_assoc,\n",
    "                    \"lift\": row['lift'],\n",
    "                    \"confidence\": row['confidence'],\n",
    "                    \"support\": row['support']\n",
    "                })\n",
    "                # Category summary\n",
    "                key = (cat_from, cat_to)\n",
    "                if key not in cat_summary:\n",
    "                    cat_summary[key] = []\n",
    "                cat_summary[key].append(row['lift'])\n",
    "    product_category_reco = pd.DataFrame(reco)\n",
    "    # Category summary\n",
    "    summary_rows = []\n",
    "    for (cat_from, cat_to), lifts in cat_summary.items():\n",
    "        avg_lift = np.mean(lifts)\n",
    "        summary_rows.append({\n",
    "            \"category_from\": cat_from,\n",
    "            \"category_to\": cat_to,\n",
    "            \"num_product_pairs\": len(lifts),\n",
    "            \"avg_lift\": avg_lift\n",
    "        })\n",
    "    category_summary = pd.DataFrame(summary_rows).sort_values('avg_lift', ascending=False)\n",
    "    return category_summary, product_category_reco\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. Run Full Pipeline\n",
    "# -------------------------------------------------------\n",
    "def run_apriori_pipeline(min_support=0.005, min_confidence=0.05, max_len=None, use_cache=True):\n",
    "    try:\n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(\"ðŸš€ STARTING ENHANCED APRIORI PIPELINE\")\n",
    "        logger.info(\"=\"*70)\n",
    "        df = pd.read_csv(FEATURE_PATH)\n",
    "        df = validate_data(df)\n",
    "        print_data_quality_report(df)\n",
    "        txn = create_transaction_matrix(df, use_cache)\n",
    "        frequent_itemsets, rules = run_apriori(txn, min_support, min_confidence, max_len)\n",
    "        category_summary, product_category_reco = generate_hierarchical_recommendations(df, rules)\n",
    "        \n",
    "        logger.info(\"âœ… PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        return {\n",
    "            'rules': rules,\n",
    "            'category_summary': category_summary,\n",
    "            'product_category_reco': product_category_reco\n",
    "        }\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"âŒ Pipeline failed: Input file not found at {FEATURE_PATH}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Pipeline failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. Run the Pipeline\n",
    "# -------------------------------------------------------\n",
    "# Adjust min_support if you get \"No frequent itemsets found\"\n",
    "results = run_apriori_pipeline(\n",
    "    min_support=0.005,  # 0.5% of all transactions\n",
    "    min_confidence=0.05, # 5% confidence\n",
    "    use_cache=False      # Set to False to rebuild matrix\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7. Analyze and Visualize Results (THE DIAGRAMS)\n",
    "# -------------------------------------------------------\n",
    "if results and not results['rules'].empty:\n",
    "    rules = results['rules']\n",
    "    category_summary = results['category_summary']\n",
    "\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"ANALYSIS: PLOTTING APRIORI DIAGRAMS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"Top 20 Rules by Lift:\")\n",
    "    display(rules.sort_values('lift', ascending=False).head(20))\n",
    "\n",
    "    # --- Diagram 1: Rule Scatter Plot ---\n",
    "    print(\"\\n--- Diagram 1: Rule Scatter Plot (Support vs. Confidence) ---\")\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.scatterplot(\n",
    "        x='support', \n",
    "        y='confidence', \n",
    "        size='lift', \n",
    "        data=rules[rules['lift'] > 1.5],  # Only plot rules with lift > 1.5\n",
    "        hue='lift',\n",
    "        palette='viridis',\n",
    "        sizes=(20, 500),\n",
    "        alpha=0.7\n",
    "    )\n",
    "    plt.title('Association Rules (Lift > 1.5)\\nSupport vs. Confidence, Sized by Lift')\n",
    "    plt.xlabel('Support')\n",
    "    plt.ylabel('Confidence')\n",
    "    plt.legend(title='Lift', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Diagram 2: Category Heatmap ---\n",
    "    if not category_summary.empty:\n",
    "        print(\"\\n--- Diagram 2: Category-to-Category Lift Heatmap ---\")\n",
    "        \n",
    "        # Pivot for heatmap\n",
    "        category_pivot = category_summary.pivot_table(\n",
    "            index='category_from', \n",
    "            columns='category_to', \n",
    "            values='avg_lift',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Limit to top N categories for readability\n",
    "        top_cats_from = category_summary['category_from'].value_counts().index[:15]\n",
    "        top_cats_to = category_summary['category_to'].value_counts().index[:15]\n",
    "        \n",
    "        category_pivot_filtered = category_pivot.loc[\n",
    "            category_pivot.index.isin(top_cats_from),\n",
    "            category_pivot.columns.isin(top_cats_to)\n",
    "        ]\n",
    "\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        sns.heatmap(\n",
    "            category_pivot_filtered, \n",
    "            annot=True, \n",
    "            fmt=\".2f\", \n",
    "            cmap=\"viridis\", \n",
    "            linewidths=0.5,\n",
    "            cbar_kws={'label': 'Average Lift'}\n",
    "        )\n",
    "        plt.title('Heatmap of Average Lift Between Top Product Categories')\n",
    "        plt.xlabel('Recommended Category (\"Consequent\")')\n",
    "        plt.ylabel('Original Category (\"Antecedent\")')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping category heatmap (no category data found or no rules generated).\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n--- No results to plot. ---\")\n",
    "    print(\"This likely means 'min_support' was too high and no rules were found.\")\n",
    "    print(\"Try lowering 'min_support' in the 'run_apriori_pipeline' call (e.g., to 0.001).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
